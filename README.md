# ğŸ“˜ RTL â€“RAG Document Assistant  
*A fast, simple, fully local PDF Question-Answering system using FastAPI, FAISS, and Ollama.*

---

## ğŸš€ Overview

**RTL** is a lightweight local Retrieval-Augmented Generation (RAG) system that allows you to:

- ğŸ“¤ Upload **any PDF document**
- â“ Ask natural-language questions
- ğŸ¤– Get LLM-generated answers **based only on the PDF**
- ğŸ“„ See which **page numbers** were used
- ğŸ”’ Run everything **offline** using **Ollama (Llama3)**

No cloud.  
No API keys.  
No privacy risks.

---

## ğŸ—‚ï¸ Project Structure
RTL/
â”‚
â”œâ”€â”€ app.py # FastAPI backend + endpoints
â”œâ”€â”€ rag_pipeline.py # RAG workflow: retrieval + LLM
â”œâ”€â”€ retriever.py # FAISS vector search over embeddings
â”œâ”€â”€ load_pdf.py # Extract text + build embeddings for each page
â”œâ”€â”€ embedder.py # MiniLM sentence embeddings
â”œâ”€â”€ llm_answer.py # Calls Ollama model through local HTTP API
â”œâ”€â”€ static/index.html # Simple front-end UI
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ generate_cert.py # Optional: create HTTPS certificates


---
 ğŸ› ï¸ Installation

### 1ï¸âƒ£ Clone the Repository

git clone https://github.com/shrich2406/RTL.git
cd RTL

2ï¸âƒ£ Create & Activate a Virtual Environment
python -m venv .venv
.\.venv\Scripts\Activate

3ï¸âƒ£ Install Python Dependencies
pip install -r requirements.txt

ğŸ¤– Install & Run Ollama

Download: https://ollama.com/download

Start the Ollama server:

ollama serve


Pull a model:

ollama pull llama3

â–¶ï¸ Run the Application
Start FastAPI (HTTP mode)
uvicorn app:app --reload

Or run with HTTPS (optional)
uvicorn app:app --ssl-keyfile=key.pem --ssl-certfile=cert.pem

ğŸŒ Open the Web UI

Visit:

http://localhost:8000/static/index.html


Then:

Upload a PDF

Ask a question

Receive an answer + page references

